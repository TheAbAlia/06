{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Utility\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport re\n\n#Natural Language Toolkit\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n#sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n#W2V\nimport gensim\nfrom gensim.models import KeyedVectors\n\n#Keras\nfrom tensorflow import keras \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras.models import Sequential\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-04T17:45:46.639701Z","iopub.execute_input":"2022-02-04T17:45:46.640232Z","iopub.status.idle":"2022-02-04T17:45:48.722859Z","shell.execute_reply.started":"2022-02-04T17:45:46.640139Z","shell.execute_reply":"2022-02-04T17:45:48.721959Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndf = pd.read_csv(\"../input/sentiment140/training.1600000.processed.noemoticon.csv\", \n                 encoding =\"ISO-8859-1\" , \n                 names=columns)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T17:45:48.724507Z","iopub.execute_input":"2022-02-04T17:45:48.724785Z","iopub.status.idle":"2022-02-04T17:45:52.511189Z","shell.execute_reply.started":"2022-02-04T17:45:48.724748Z","shell.execute_reply":"2022-02-04T17:45:52.510393Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#01. Pre-Processing:\n#01.00.\npositive = \"positive\"\nnegative = \"negative\"\nneutral = \"neutral\"\n\ndecode_map = {0: \"negative\", 2: \"neutral\", 4: \"positive\"}\n\ndef decode_sentiment(label):\n    return decode_map[int(label)]\ndf['target'] = df.target.apply(lambda x: decode_sentiment(x))\n\n#01.01.\nstop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\ntxt_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\ndef preprocess(text, stem=False):\n    text = re.sub(txt_re, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\ndf['text'] = df['text'].apply(lambda x: preprocess(x))\n\n#01.02.\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T17:45:52.512724Z","iopub.execute_input":"2022-02-04T17:45:52.513004Z","iopub.status.idle":"2022-02-04T17:46:47.462779Z","shell.execute_reply.started":"2022-02-04T17:45:52.512966Z","shell.execute_reply":"2022-02-04T17:46:47.461987Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#02. Processing: \n   \n#02.01.\ndocuments = [_text.split() for _text in df_train.text] \nw2v_model = gensim.models.word2vec.Word2Vec(vector_size=300, \n                                            window=7, \n                                            min_count=10, \n                                            workers=8)\nw2v_model.build_vocab(documents)\n\nwords = list(w2v_model.wv.index_to_key)\nvocab_size = len(words)\nw2v_model.train(documents, \n                total_examples=len(documents), \n                epochs=32)\n\n#02.02.\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.text)\nvocab_size = len(tokenizer.word_index) + 1\n\n#02.03.\nx_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=300)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=300)\n\n#02.04\nlabels = df_train.target.unique().tolist()\nlabels.append(neutral)\n\nencoder = LabelEncoder()\nencoder.fit(df_train.target.tolist())\n\ny_train = encoder.transform(df_train.target.tolist())\ny_test = encoder.transform(df_test.target.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T17:46:47.465009Z","iopub.execute_input":"2022-02-04T17:46:47.465287Z","iopub.status.idle":"2022-02-04T17:58:09.028668Z","shell.execute_reply.started":"2022-02-04T17:46:47.465251Z","shell.execute_reply":"2022-02-04T17:58:09.027914Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Test on example (love)\nprint(\"associated with love\", w2v_model.wv.most_similar(\"love\"), \"\\n\")\nprint(\"associated with hate\", w2v_model.wv.most_similar(\"hate\"), \"\\n\")\nprint(\"associated with cute\", w2v_model.wv.most_similar(\"cute\"), \"\\n\")\nprint(\"associated with fun\", w2v_model.wv.most_similar(\"fun\"), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-02-04T17:58:09.030085Z","iopub.execute_input":"2022-02-04T17:58:09.030341Z","iopub.status.idle":"2022-02-04T17:58:09.065089Z","shell.execute_reply.started":"2022-02-04T17:58:09.030308Z","shell.execute_reply":"2022-02-04T17:58:09.062195Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#03. Model:\n#03.01.\nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n  if word in w2v_model.wv:\n    embedding_matrix[i] = w2v_model.wv[word]\n\n\nembedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\n#03.02.\nmodel.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])\n\n#03.03. \ncallbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n\n#03.03.\nhistory = model.fit(x_train, y_train,\n                    batch_size=1024,\n                    epochs=3,\n                    validation_split=0.1,\n                    verbose=1,\n                    callbacks=callbacks)\n#03.04.\nscore = model.evaluate(x_test, y_test, batch_size=1024)\n\n#03.05.\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\ndef decode_sentiment(score, include_neutral=True):\n    if include_neutral:        \n        label = neutral\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            label = negative\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            label = positive\n        return label\n    else:\n        return negative if score < 0.5 else positive\n\ndef predict(text, include_neutral=True):\n    start_at = time.time()\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=300)\n    score = model.predict([x_test])[0]\n    label = decode_sentiment(score, include_neutral=include_neutral)\n    return {\"label\": label, \"score\": float(score), \"elapsed_time\": time.time()-start_at}  ","metadata":{"execution":{"iopub.status.busy":"2022-02-04T18:32:04.375361Z","iopub.execute_input":"2022-02-04T18:32:04.375616Z","iopub.status.idle":"2022-02-04T19:02:56.027106Z","shell.execute_reply.started":"2022-02-04T18:32:04.375586Z","shell.execute_reply":"2022-02-04T19:02:56.026378Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#04.Classification report on a confusion matrix.\n\ny_pred_1d = []\ny_test_1d = list(df_test.target)\nscores = model.predict(x_test, verbose=1, batch_size=8000)\ny_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]\n\n\nprint(classification_report(y_test_1d, y_pred_1d))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T19:04:51.321053Z","iopub.execute_input":"2022-02-04T19:04:51.321381Z","iopub.status.idle":"2022-02-04T19:05:10.653487Z","shell.execute_reply.started":"2022-02-04T19:04:51.321346Z","shell.execute_reply":"2022-02-04T19:05:10.652718Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import time\n\ntwt= \"Hands down the best chicken wings I have ever eaten. My partner and I were visiting some friends in the area and these heavenly wings (and a couple of sides of rice) were the only thing requested.\"\n\npredict(twt, True)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T19:06:47.270800Z","iopub.execute_input":"2022-02-04T19:06:47.271584Z","iopub.status.idle":"2022-02-04T19:06:47.402063Z","shell.execute_reply.started":"2022-02-04T19:06:47.271542Z","shell.execute_reply":"2022-02-04T19:06:47.401312Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}